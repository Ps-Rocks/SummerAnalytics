{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f02f4e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (2845, 29)\n",
      "Columns: ['Unnamed: 0', 'ID', '20150720_N', '20150602_N', '20150517_N', '20150501_N', '20150415_N', '20150330_N', '20150314_N', '20150226_N', '20150210_N', '20150125_N', '20150109_N', '20141117_N', '20141101_N', '20141016_N', '20140930_N', '20140813_N', '20140626_N', '20140610_N', '20140525_N', '20140509_N', '20140423_N', '20140407_N', '20140322_N', '20140218_N', '20140202_N', '20140117_N', '20140101_N']\n",
      "\n",
      "First few rows:\n",
      "   Unnamed: 0  ID  20150720_N  20150602_N  20150517_N  20150501_N  20150415_N  \\\n",
      "0           0   1     7466.42     413.162     5761.00     5625.45     489.403   \n",
      "1           1   2     7235.26    6037.350     1027.56     6085.14    1618.050   \n",
      "2           2   3     7425.08    6969.980     1177.94     7408.93     861.061   \n",
      "3           3   4     7119.12    1731.620     6311.93     6441.61     465.979   \n",
      "4           4   5     7519.55    8130.260     1482.54     7879.53    1001.210   \n",
      "\n",
      "   20150330_N  20150314_N  20150226_N  ...  20140610_N  20140525_N  \\\n",
      "0     3923.84    3097.110     6766.42  ...     801.184     927.115   \n",
      "1     6668.54    2513.990     1051.69  ...    5533.470    5103.040   \n",
      "2     7644.43     814.458     1504.29  ...    1981.390    6204.540   \n",
      "3     7128.42    1649.120     6935.22  ...     959.344    5794.150   \n",
      "4     7937.60    4122.530     1094.51  ...    7636.070    6996.760   \n",
      "\n",
      "   20140509_N  20140423_N  20140407_N  20140322_N  20140218_N  20140202_N  \\\n",
      "0     4704.14     6378.42     340.949    2695.570     527.268     4736.75   \n",
      "1     5216.12     4885.27    4366.790    1234.140    3298.110     6942.68   \n",
      "2     7021.69     5704.41    4897.450    1789.990    2206.100     6928.93   \n",
      "3     1045.57     5572.90     586.287     685.906    1287.000     6734.72   \n",
      "4     7413.43     4596.13    4511.700    1413.520    3283.940     7937.68   \n",
      "\n",
      "   20140117_N  20140101_N  \n",
      "0     601.843    6639.760  \n",
      "1    1070.440     842.101  \n",
      "2    1036.560     831.441  \n",
      "3     824.584    6883.610  \n",
      "4    1857.800    1336.920  \n",
      "\n",
      "[5 rows x 29 columns]\n",
      "\n",
      "Created 35 features:\n",
      "['mean_ndvi', 'std_ndvi', 'median_ndvi', 'min_ndvi', 'max_ndvi', 'range_ndvi', 'cv_ndvi', 'p25_ndvi', 'p75_ndvi', 'p90_ndvi', 'trend_slope', 'season_0_mean', 'season_0_std', 'season_1_mean', 'season_1_std', 'season_2_mean', 'season_2_std', 'season_3_mean', 'season_3_std', 'mean_diff', 'std_diff', 'max_positive_change', 'max_negative_change', 'fft_mag_0', 'fft_mag_1', 'fft_mag_2', 'fft_mag_3', 'fft_mag_4', 'vegetation_vigor', 'growing_season_length', 'missing_count', 'missing_ratio', 'stability_mean', 'stability_max', 'peak_count']\n",
      "\n",
      "Feature statistics:\n",
      "         mean_ndvi     std_ndvi  median_ndvi     min_ndvi     max_ndvi  \\\n",
      "count  2845.000000  2845.000000  2845.000000  2845.000000  2845.000000   \n",
      "mean   3330.786235  2210.528927  3228.304978   272.211840  6859.641979   \n",
      "std    1267.458583   690.666764  1731.336895   722.086948  1891.078058   \n",
      "min   -2575.879248   220.687891 -2605.880000 -6807.550000   338.410000   \n",
      "25%    2700.097333  1894.217328  1840.480000   268.213000  6655.470000   \n",
      "50%    3421.766333  2379.213426  3090.320000   360.170000  7643.070000   \n",
      "75%    4254.555074  2729.918992  4563.720000   486.698000  8051.600000   \n",
      "max    6119.885556  3352.329086  7824.690000  1379.120000  8611.450000   \n",
      "\n",
      "         range_ndvi      cv_ndvi     p25_ndvi     p75_ndvi     p90_ndvi  ...  \\\n",
      "count   2845.000000  2845.000000  2845.000000  2845.000000  2845.000000  ...   \n",
      "mean    6587.430139     0.684876  1403.576510  5169.194261  6177.858730  ...   \n",
      "std     1664.896292     0.712190   924.468972  1933.808575  1919.384586  ...   \n",
      "min      743.554000   -28.406576 -4031.820000 -1434.200000   -67.205220  ...   \n",
      "25%     6214.657000     0.541854   792.729500  4009.605000  5551.732000  ...   \n",
      "50%     7237.608000     0.639305  1276.140000  5444.655000  6859.994000  ...   \n",
      "75%     7659.160000     0.772253  1942.275000  6807.645000  7596.056000  ...   \n",
      "max    10984.260000     9.515579  5220.480000  8190.645000  8454.766000  ...   \n",
      "\n",
      "          fft_mag_2     fft_mag_3     fft_mag_4  vegetation_vigor  \\\n",
      "count   2845.000000   2845.000000   2845.000000       2845.000000   \n",
      "mean   10471.076748  11377.000468   8913.528724       6587.430139   \n",
      "std     6064.834502   6605.117293   4994.386567       1664.896292   \n",
      "min      219.829651    143.642932     45.478841        743.554000   \n",
      "25%     5661.254520   6293.339278   5216.259969       6214.657000   \n",
      "50%     9601.819590  10876.545281   8298.945742       7237.608000   \n",
      "75%    14253.056378  15429.635415  11949.532776       7659.160000   \n",
      "max    36502.201039  44581.551206  28677.969999      10984.260000   \n",
      "\n",
      "       growing_season_length  missing_count  missing_ratio  stability_mean  \\\n",
      "count            2845.000000         2845.0         2845.0     2845.000000   \n",
      "mean               13.130756            0.0            0.0     1839.490726   \n",
      "std                 2.399115            0.0            0.0      641.559176   \n",
      "min                 6.000000            0.0            0.0      172.645848   \n",
      "25%                11.000000            0.0            0.0     1466.784975   \n",
      "50%                13.000000            0.0            0.0     1925.403292   \n",
      "75%                15.000000            0.0            0.0     2308.732007   \n",
      "max                20.000000            0.0            0.0     3343.508878   \n",
      "\n",
      "       stability_max   peak_count  \n",
      "count    2845.000000  2845.000000  \n",
      "mean     3395.383956     8.166257  \n",
      "std       966.015463     1.234234  \n",
      "min       359.296266     4.000000  \n",
      "25%      3038.137619     7.000000  \n",
      "50%      3714.906306     8.000000  \n",
      "75%      4059.708776     9.000000  \n",
      "max      4809.165114    12.000000  \n",
      "\n",
      "[8 rows x 35 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.impute import SimpleImputer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class NDVILandCoverClassifier:\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "        self.imputer = SimpleImputer(strategy='median')\n",
    "        self.model = LogisticRegression(\n",
    "            max_iter=1000,\n",
    "            random_state=42,\n",
    "            class_weight='balanced'\n",
    "        )\n",
    "        self.feature_names = []\n",
    "        \n",
    "    def create_features(self, df):\n",
    "        \"\"\"Create comprehensive features from NDVI time series\"\"\"\n",
    "        features = pd.DataFrame()\n",
    "        \n",
    "        # Get NDVI columns (exclude ID and class if present)\n",
    "        ndvi_cols = [col for col in df.columns if col.endswith('_N')]\n",
    "        ndvi_data = df[ndvi_cols].copy()\n",
    "        \n",
    "        # 1. Basic statistical features\n",
    "        features['mean_ndvi'] = ndvi_data.mean(axis=1)\n",
    "        features['std_ndvi'] = ndvi_data.std(axis=1)\n",
    "        features['median_ndvi'] = ndvi_data.median(axis=1)\n",
    "        features['min_ndvi'] = ndvi_data.min(axis=1)\n",
    "        features['max_ndvi'] = ndvi_data.max(axis=1)\n",
    "        features['range_ndvi'] = features['max_ndvi'] - features['min_ndvi']\n",
    "        features['cv_ndvi'] = features['std_ndvi'] / (features['mean_ndvi'] + 1e-8)\n",
    "        \n",
    "        # 2. Percentile features\n",
    "        for p in [25, 75, 90]:\n",
    "            features[f'p{p}_ndvi'] = ndvi_data.quantile(p/100, axis=1)\n",
    "        \n",
    "        # 3. Temporal trend features\n",
    "        time_points = np.arange(len(ndvi_cols))\n",
    "        trends = []\n",
    "        for idx in range(len(df)):\n",
    "            values = ndvi_data.iloc[idx].values\n",
    "            valid_mask = ~np.isnan(values)\n",
    "            if np.sum(valid_mask) > 2:\n",
    "                slope = np.polyfit(time_points[valid_mask], values[valid_mask], 1)[0]\n",
    "            else:\n",
    "                slope = 0\n",
    "            trends.append(slope)\n",
    "        features['trend_slope'] = trends\n",
    "        \n",
    "        # 4. Seasonal features (assuming roughly monthly data)\n",
    "        # Split into seasons (approximate)\n",
    "        season_size = len(ndvi_cols) // 4\n",
    "        for i in range(4):\n",
    "            start_idx = i * season_size\n",
    "            end_idx = (i + 1) * season_size if i < 3 else len(ndvi_cols)\n",
    "            season_cols = ndvi_cols[start_idx:end_idx]\n",
    "            features[f'season_{i}_mean'] = ndvi_data[season_cols].mean(axis=1)\n",
    "            features[f'season_{i}_std'] = ndvi_data[season_cols].std(axis=1)\n",
    "        \n",
    "        # 5. Difference features (rate of change)\n",
    "        ndvi_diff = ndvi_data.diff(axis=1)\n",
    "        features['mean_diff'] = ndvi_diff.mean(axis=1)\n",
    "        features['std_diff'] = ndvi_diff.std(axis=1)\n",
    "        features['max_positive_change'] = ndvi_diff.max(axis=1)\n",
    "        features['max_negative_change'] = ndvi_diff.min(axis=1)\n",
    "        \n",
    "        # 6. Spectral features (FFT-based)\n",
    "        fft_features = []\n",
    "        for idx in range(len(df)):\n",
    "            values = ndvi_data.iloc[idx].values\n",
    "            valid_mask = ~np.isnan(values)\n",
    "            if np.sum(valid_mask) > 4:\n",
    "                # Interpolate missing values for FFT\n",
    "                interp_values = np.interp(\n",
    "                    np.arange(len(values)),\n",
    "                    np.where(valid_mask)[0],\n",
    "                    values[valid_mask]\n",
    "                )\n",
    "                fft = np.fft.fft(interp_values)\n",
    "                # Take magnitude of first few frequency components\n",
    "                fft_mag = np.abs(fft[:5])\n",
    "                fft_features.append(fft_mag)\n",
    "            else:\n",
    "                fft_features.append(np.zeros(5))\n",
    "        \n",
    "        fft_array = np.array(fft_features)\n",
    "        for i in range(5):\n",
    "            features[f'fft_mag_{i}'] = fft_array[:, i]\n",
    "        \n",
    "        # 7. Vegetation index patterns\n",
    "        features['vegetation_vigor'] = features['max_ndvi'] - features['min_ndvi']\n",
    "        features['growing_season_length'] = (ndvi_data > features['mean_ndvi'].values.reshape(-1, 1)).sum(axis=1)\n",
    "        \n",
    "        # 8. Missing data patterns (important for noisy data)\n",
    "        features['missing_count'] = ndvi_data.isnull().sum(axis=1)\n",
    "        features['missing_ratio'] = features['missing_count'] / len(ndvi_cols)\n",
    "        \n",
    "        # 9. Stability features\n",
    "        rolling_std = ndvi_data.rolling(window=3, axis=1).std()\n",
    "        features['stability_mean'] = rolling_std.mean(axis=1)\n",
    "        features['stability_max'] = rolling_std.max(axis=1)\n",
    "        \n",
    "        # 10. Peak detection features\n",
    "        peak_counts = []\n",
    "        for idx in range(len(df)):\n",
    "            values = ndvi_data.iloc[idx].values\n",
    "            valid_mask = ~np.isnan(values)\n",
    "            if np.sum(valid_mask) > 3:\n",
    "                valid_values = values[valid_mask]\n",
    "                # Simple peak detection\n",
    "                peaks = 0\n",
    "                for i in range(1, len(valid_values) - 1):\n",
    "                    if valid_values[i] > valid_values[i-1] and valid_values[i] > valid_values[i+1]:\n",
    "                        peaks += 1\n",
    "                peak_counts.append(peaks)\n",
    "            else:\n",
    "                peak_counts.append(0)\n",
    "        features['peak_count'] = peak_counts\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def preprocess_data(self, X, fit=True):\n",
    "        \"\"\"Handle missing values and scaling\"\"\"\n",
    "        if fit:\n",
    "            X_imputed = self.imputer.fit_transform(X)\n",
    "            X_scaled = self.scaler.fit_transform(X_imputed)\n",
    "        else:\n",
    "            X_imputed = self.imputer.transform(X)\n",
    "            X_scaled = self.scaler.transform(X_imputed)\n",
    "        \n",
    "        return X_scaled\n",
    "    \n",
    "    def fit(self, df, target_col='class'):\n",
    "        \"\"\"Train the model\"\"\"\n",
    "        # Create features\n",
    "        X = self.create_features(df)\n",
    "        y = df[target_col]\n",
    "        \n",
    "        # Store feature names\n",
    "        self.feature_names = X.columns.tolist()\n",
    "        \n",
    "        # Preprocess\n",
    "        X_processed = self.preprocess_data(X, fit=True)\n",
    "        \n",
    "        # Train model\n",
    "        self.model.fit(X_processed, y)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict(self, df):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        X = self.create_features(df)\n",
    "        X_processed = self.preprocess_data(X, fit=False)\n",
    "        return self.model.predict(X_processed)\n",
    "    \n",
    "    def predict_proba(self, df):\n",
    "        \"\"\"Get prediction probabilities\"\"\"\n",
    "        X = self.create_features(df)\n",
    "        X_processed = self.preprocess_data(X, fit=False)\n",
    "        return self.model.predict_proba(X_processed)\n",
    "\n",
    "# Load and prepare your test data\n",
    "def load_data(file_path):\n",
    "    \"\"\"Load the hackathon data\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df\n",
    "\n",
    "# Example usage with your test data\n",
    "def main():\n",
    "    # Load your test data\n",
    "    test_df = pd.read_csv('hacktest.csv')  # Your uploaded file\n",
    "    \n",
    "    print(\"Dataset shape:\", test_df.shape)\n",
    "    print(\"Columns:\", test_df.columns.tolist())\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    print(test_df.head())\n",
    "    \n",
    "    # Since this is test data without labels, let's create a demo\n",
    "    # In the actual competition, you'd have training data with labels\n",
    "    \n",
    "    # For demonstration, let's assume you have training data\n",
    "    # You would replace this with your actual training data loading\n",
    "    \n",
    "    # Create classifier\n",
    "    classifier = NDVILandCoverClassifier()\n",
    "    \n",
    "    # Create features for analysis\n",
    "    features = classifier.create_features(test_df)\n",
    "    print(f\"\\nCreated {len(features.columns)} features:\")\n",
    "    print(features.columns.tolist())\n",
    "    print(f\"\\nFeature statistics:\")\n",
    "    print(features.describe())\n",
    "    \n",
    "    # If you had training data, you would do:\n",
    "    # classifier.fit(train_df, 'class')\n",
    "    # predictions = classifier.predict(test_df)\n",
    "    \n",
    "    return classifier, features\n",
    "\n",
    "# Cross-validation function for when you have training data\n",
    "def evaluate_model(train_df, n_folds=5):\n",
    "    \"\"\"Evaluate model performance using cross-validation\"\"\"\n",
    "    classifier = NDVILandCoverClassifier()\n",
    "    \n",
    "    # Create features\n",
    "    X = classifier.create_features(train_df)\n",
    "    y = train_df['class']\n",
    "    \n",
    "    # Preprocess\n",
    "    X_processed = classifier.preprocess_data(X, fit=True)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(classifier.model, X_processed, y, cv=cv, scoring='accuracy')\n",
    "    \n",
    "    print(f\"Cross-validation scores: {scores}\")\n",
    "    print(f\"Mean CV accuracy: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})\")\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Feature importance analysis\n",
    "def analyze_feature_importance(classifier, feature_names):\n",
    "    \"\"\"Analyze which features are most important\"\"\"\n",
    "    if hasattr(classifier.model, 'coef_'):\n",
    "        # For logistic regression, we can look at coefficient magnitudes\n",
    "        coef_abs = np.abs(classifier.model.coef_).mean(axis=0)\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': coef_abs\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        print(\"Top 10 most important features:\")\n",
    "        print(feature_importance.head(10))\n",
    "        \n",
    "        return feature_importance\n",
    "    \n",
    "    return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    classifier, features = main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd1b8eb",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cf3accfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"hacktrain.csv\")\n",
    "test_df = pd.read_csv(\"hacktest.csv\")\n",
    "classifier = NDVILandCoverClassifier()\n",
    "classifier.fit(train_df, 'class')\n",
    "predictions = classifier.predict(test_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c231e948",
   "metadata": {},
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6c7490b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [0.726875 0.72     0.75125  0.74375  0.7125  ]\n",
      "Mean CV accuracy: 0.7309 (+/- 0.0290)\n"
     ]
    }
   ],
   "source": [
    "scores = evaluate_model(train_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2d7261",
   "metadata": {},
   "source": [
    "## Feature-Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "650c020a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 most important features:\n",
      "          feature  importance\n",
      "1        std_ndvi    2.805326\n",
      "0       mean_ndvi    1.669619\n",
      "8        p75_ndvi    1.503018\n",
      "11  season_0_mean    1.046259\n",
      "16   season_2_std    0.982182\n",
      "17  season_3_mean    0.978139\n",
      "20       std_diff    0.859199\n",
      "10    trend_slope    0.852722\n",
      "13  season_1_mean    0.771040\n",
      "9        p90_ndvi    0.670704\n"
     ]
    }
   ],
   "source": [
    "importance = analyze_feature_importance(classifier, classifier.feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99a7b78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created: submission.csv\n"
     ]
    }
   ],
   "source": [
    "# Final step: Save predictions to CSV\n",
    "submission = pd.DataFrame({\n",
    "    'ID': test_df['ID'],\n",
    "    'class': predictions\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Submission file created: submission.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
